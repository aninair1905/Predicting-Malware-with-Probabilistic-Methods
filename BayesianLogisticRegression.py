import numpy as np
import pymc as pm
from sklearn.base import ClassifierMixin, BaseEstimator
from sklearn.metrics import average_precision_score
from scipy.special import expit

class BayesianLogisticRegression(ClassifierMixin, BaseEstimator):
    ''' 
    BayesianLogisticRegression for binary classification. Implemented using
    pymc.find_MAP, with Gaussian/Laplacian prior and Bernoulli likelihood.

    Initial Parameters:
     - penalty : str, can be 'L2' or 'L1
     - reg_strength : int, the strength of the regularization penalty
     - fit_intercept : bool, whether or not to add a bias before the sigmoid
     - random_state : None, int, or np.random.RandomState() instance, for 
        shuffling the training data
     - max_iter : int, number of training iterations for MAP estimation.
    
    Methods:
      - fit : fit a logistic regression to an X and y
      - predict_proba : predict class probability score for an X
      - predict : predict class labels for an X
      - score : score a model on an X and y (using area under precision recall curve)
    '''

    def __init__(self, penalty='L2', reg_strength=1.0, fit_intercept=True, 
                 random_state=None, max_iter=5000, verbose=False):
        
        self.penalty = penalty
        self.reg_strength = reg_strength
        self.fit_intercept = fit_intercept
        self.max_iter = max_iter
        self.verbose = verbose
        self.random_state = random_state


    def _validate_params(self):

        self.penalty = str(self.penalty)
        self.reg_strength = float(self.reg_strength)
        self.fit_intercept = bool(self.fit_intercept)
        self.max_iter = int(self.max_iter)
        self.verbose = bool(self.verbose)

        if self.penalty == 'L2':
            self._prior = pm.Normal
        elif self.penalty == 'L1':
            self._prior = pm.Laplace
        else:
            raise ValueError(f'\'penalty\' must be \'L2\' or \'L1\'')

        if self.random_state is None:
            self.random_state = np.random.RandomState()
        elif isinstance(self.random_state, int):
            self.random_state = np.random.RandomState(self.random_state)
        elif isinstance(self.random_state, np.random.RandomState):
            pass
        else:
            raise ValueError("\'random_state\' must be an integer, a RandomState instance, or None.")
    

    def fit(self, x_NF, y_N):
        ''' 
        fit a logistic regression to x_NF and y_N

        x_NF is of shape (n_samples, n_features)
        y_N is of shape (n_samples,)
        '''
        self._validate_params()

        N, F = x_NF.shape
        assert y_N.shape[0] == N

        idxs = np.arange(N)
        self.random_state.shuffle(idxs)
        x_NF = x_NF[idxs, :]
        y_N = y_N[idxs]

        if self.fit_intercept:
            x_NF = np.hstack((x_NF, np.ones((N, 1))))

        with pm.Model() as self._pm_model:
            initial_beta = self.random_state.randn(x_NF.shape[1]) * 0.01
            weights = self._prior('beta', 0, self.reg_strength, shape=x_NF.shape[1])
            
            linear = pm.math.dot(x_NF, weights) + 1e-6
            likelihood = pm.invlogit(linear)
            
            pm.Bernoulli('logit', p=likelihood, observed=y_N)
            
            # Find MAP estimate, getting the weights here
            self._weights = pm.find_MAP(progressbar=self.verbose)['beta']

        self.classes_ = np.unique(y_N)

        return self
    
    def predict_proba(self, x_MF):
        ''' 
        predict class probabilities for x_MF

        x_MF is of shape (n_samples, n_features)

        returns a value of shape (n_samples, n_classes) (where n_classes == 2)
        '''

        M, F = x_MF.shape
        if self.fit_intercept:
            x_MF = np.hstack((x_MF, np.ones((M, 1))))

        class1_probas = expit(np.dot(x_MF, self._weights)).reshape(-1, 1)
        return np.hstack((1 - class1_probas, class1_probas))
    
    def predict(self, x_MF):
        ''' 
        predict class labels (0 or 1) for x_MF

        x_MF is of shape (n_samples, n_features)

        returns a value of shape (n_samples,)
        '''

        return self.predict_proba(x_MF)[:, 1] > 0.5
    
    def score(self, x_MF, y_M, score_func, probas=True, **score_func_params):
        ''' 
        score a model on x_MF and y_M using the provided score function

        score functions either take predictions as probabilities or binary vals,
        so the 'probas' must be selected to match what the score function takes

        additional parameters to the score function can be passed through
        **score_func_params

        e.g. model.score(X, y, average_precision_score, probas=True, 
                         **{'average' : 'weighted'}))
        '''
        if probas:
            y_pred_M = self.predict_proba(x_MF)[:, 1]
        else:
            y_pred_M = self.predict(x_MF)
        return score_func(y_M, y_pred_M, **score_func_params)

if __name__ == "__main__":

    from sklearn.metrics import average_precision_score
    model = BayesianLogisticRegression()
    X = np.random.rand(100, 10)
    y = np.random.rand(100) > 0.5
    y = y.astype(int)

    print('fitting!')
    model.fit(X, y)
    print('Score:', model.score(X, y, average_precision_score, probas=True, **{'average' : 'weighted'}))